{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba6e3fe8",
   "metadata": {},
   "source": [
    "# Final Social Pricing Optimization\n",
    "Notebook version for Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa02249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import time\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "try:\n",
    "    tqdm = importlib.import_module('tqdm.auto').tqdm\n",
    "except ModuleNotFoundError:\n",
    "    def tqdm(iterable, **kwargs):\n",
    "        return iterable\n",
    "\n",
    "\n",
    "global_start = time.time()\n",
    "\n",
    "# 1. Setup Directory\n",
    "SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "BASE_DIR = os.path.dirname(SCRIPT_DIR) if os.path.basename(SCRIPT_DIR).lower() == 'xgboost' else SCRIPT_DIR\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'XGBoost')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f'[1/7] Output directory initialized at {OUTPUT_DIR}')\n",
    "\n",
    "# 2. Load Data\n",
    "print('[2/7] Loading datasets...')\n",
    "files = [\n",
    "    'price_cost.csv', 'sales_history.csv', 'inventory.csv', 'competitor_prices.csv',\n",
    "    'sku_master.csv', 'store_master.csv', 'promo_log.csv', 'calendar_weather.csv',\n",
    "    'XEL.csv', 'sample_submission.csv'\n",
    "]\n",
    "\n",
    "dfs = {}\n",
    "load_start = time.time()\n",
    "for file_name in tqdm(files, desc='Loading Files'):\n",
    "    dfs[file_name.split('.')[0]] = pd.read_csv(os.path.join(BASE_DIR, file_name))\n",
    "print(f'   - Data loaded in {time.time() - load_start:.2f}s')\n",
    "\n",
    "# 3. Pre-processing\n",
    "print('[3/7] Pre-processing data...')\n",
    "df_sales = dfs['sales_history'].copy()\n",
    "df_sales['date'] = pd.to_datetime(df_sales['date'])\n",
    "\n",
    "df_template = dfs['sample_submission'].copy()\n",
    "df_template['date'] = pd.to_datetime(df_template['date'], format='%d/%m/%Y')\n",
    "\n",
    "df_price_cost = dfs['price_cost'].copy()\n",
    "df_price_cost['vat_rate'] = df_price_cost['vat_rate'].fillna(0.07)\n",
    "\n",
    "df_opt = df_template.merge(\n",
    "    df_price_cost[['sku_id', 'regular_price', 'unit_cost', 'vat_rate']], on='sku_id', how='left'\n",
    ")\n",
    "df_opt = df_opt.merge(dfs['sku_master'], on='sku_id', how='left')\n",
    "df_opt = df_opt.merge(dfs['store_master'], on='store_id', how='left')\n",
    "\n",
    "# 4. Feature Engineering\n",
    "print('[4/7] Engineering features...')\n",
    "daily_sales = df_sales.groupby(['date', 'sku_id'])['qty'].sum().reset_index()\n",
    "daily_sales['qty_lag_7'] = daily_sales.groupby('sku_id')['qty'].shift(7)\n",
    "daily_sales['qty_rmean_7'] = daily_sales.groupby('sku_id')['qty'].transform(lambda x: x.shift(7).rolling(7).mean())\n",
    "\n",
    "# 14-day demand base\n",
    "last_sales_date = df_sales['date'].max()\n",
    "cutoff_14d = last_sales_date - pd.Timedelta(days=13)\n",
    "recent_sales_14d = df_sales[df_sales['date'] >= cutoff_14d]\n",
    "avg_demand_14d = (\n",
    "    recent_sales_14d\n",
    "    .groupby(['store_id', 'sku_id'])['qty']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'qty': 'avg_daily_demand_14d'})\n",
    ")\n",
    "df_opt = df_opt.merge(avg_demand_14d, on=['store_id', 'sku_id'], how='left')\n",
    "df_opt['avg_daily_demand_14d'] = df_opt['avg_daily_demand_14d'].fillna(0)\n",
    "\n",
    "# Competitor and inventory features\n",
    "latest_comp = dfs['competitor_prices'].copy()\n",
    "latest_comp['date'] = pd.to_datetime(latest_comp['date'])\n",
    "latest_comp = latest_comp.sort_values('date').groupby('sku_id').tail(1)[['sku_id', 'comp_price']]\n",
    "df_opt = df_opt.merge(latest_comp, on='sku_id', how='left')\n",
    "df_opt['comp_price'] = df_opt['comp_price'].fillna(df_opt['regular_price'])\n",
    "\n",
    "latest_inv = dfs['inventory'].groupby('sku_id')['on_hand'].sum().reset_index()\n",
    "df_opt = df_opt.merge(latest_inv, on='sku_id', how='left').fillna({'on_hand': 0})\n",
    "\n",
    "# 14-day stockout model (rule-based)\n",
    "df_opt['expected_units_14d'] = df_opt['avg_daily_demand_14d'] * 14\n",
    "df_opt['stockout_risk_14d'] = (df_opt['on_hand'] < df_opt['expected_units_14d']).astype(int)\n",
    "\n",
    "# Latest observed selling price (for day-to-day fluctuation control)\n",
    "latest_selling_price = (\n",
    "    df_sales.sort_values('date')\n",
    "    .groupby(['store_id', 'sku_id'])\n",
    "    .tail(1)[['store_id', 'sku_id', 'price_paid']]\n",
    "    .rename(columns={'price_paid': 'last_price_paid'})\n",
    ")\n",
    "df_opt = df_opt.merge(latest_selling_price, on=['store_id', 'sku_id'], how='left')\n",
    "df_opt['last_price_paid'] = df_opt['last_price_paid'].fillna(df_opt['regular_price'])\n",
    "\n",
    "# Social category tag (essential products get stronger affordability protection)\n",
    "essential_keywords = r'water|milk|rice|egg|oil|noodle|baby|diaper|medicine|health'\n",
    "essential_mask = (\n",
    "    df_opt['category'].astype(str).str.lower().str.contains(essential_keywords, regex=True) |\n",
    "    df_opt['subcategory'].astype(str).str.lower().str.contains(essential_keywords, regex=True)\n",
    ")\n",
    "df_opt['is_essential'] = essential_mask.astype(int)\n",
    "\n",
    "# 5. Train XGBoost (kept for compatibility with existing workflow)\n",
    "print('[5/7] Training XGBoost model...')\n",
    "train_start = time.time()\n",
    "df_train = daily_sales.dropna().copy()\n",
    "X_train = df_train[['sku_id', 'qty_lag_7', 'qty_rmean_7']]\n",
    "y_train = df_train['qty']\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=6)\n",
    "model.fit(X_train, y_train)\n",
    "print(f'   - Model trained in {time.time() - train_start:.2f}s')\n",
    "\n",
    "# 6. Optimization with social-balanced objective\n",
    "print('[6/7] Running Price Optimization...')\n",
    "opt_start = time.time()\n",
    "\n",
    "MAX_DAILY_CHANGE_PCT = 0.10\n",
    "ESSENTIAL_MAX_GAP_PCT = 0.02  # essential SKUs can be at most +2% above competitor\n",
    "\n",
    "\n",
    "deltas = np.linspace(-0.20, 0.30, 11)\n",
    "reg_prices = df_opt['regular_price'].values[:, np.newaxis]\n",
    "costs = df_opt['unit_cost'].values[:, np.newaxis]\n",
    "comp_prices = df_opt['comp_price'].values[:, np.newaxis]\n",
    "vat_multiplier = (1 + df_opt['vat_rate'].values[:, np.newaxis])\n",
    "expected_units_daily = df_opt['avg_daily_demand_14d'].values[:, np.newaxis]\n",
    "last_prices = df_opt['last_price_paid'].values[:, np.newaxis]\n",
    "essential_factor = df_opt['is_essential'].values[:, np.newaxis]\n",
    "\n",
    "candidates = reg_prices * (1 + deltas)\n",
    "\n",
    "\n",
    "def apply_rounding(arr):\n",
    "    base = np.floor(arr)\n",
    "    c1, c2, c3 = base + 0.0, base + 0.5, base + 0.9\n",
    "    d1, d2, d3 = np.abs(arr - c1), np.abs(arr - c2), np.abs(arr - c3)\n",
    "    return np.where(d1 < d2, np.where(d1 < d3, c1, c3), np.where(d2 < d3, c2, c3))\n",
    "\n",
    "\n",
    "candidates = apply_rounding(candidates)\n",
    "\n",
    "# Business rules\n",
    "min_markdown_price = reg_prices * 0.70\n",
    "min_cost_vat_price = costs * vat_multiplier\n",
    "daily_floor = last_prices * (1 - MAX_DAILY_CHANGE_PCT)\n",
    "daily_ceiling = last_prices * (1 + MAX_DAILY_CHANGE_PCT)\n",
    "\n",
    "rule_markdown = candidates >= min_markdown_price\n",
    "rule_min_price = candidates >= min_cost_vat_price\n",
    "rule_daily_change = (candidates >= daily_floor) & (candidates <= daily_ceiling)\n",
    "\n",
    "# Social guardrail: essential products should not be much higher than competitor\n",
    "essential_cap = np.where(essential_factor == 1, comp_prices * (1 + ESSENTIAL_MAX_GAP_PCT), np.inf)\n",
    "rule_essential_fair = candidates <= essential_cap\n",
    "\n",
    "feasible_mask = rule_markdown & rule_min_price & rule_daily_change & rule_essential_fair\n",
    "\n",
    "candidates_before_vat = candidates / vat_multiplier\n",
    "valid_margin_mask = candidates_before_vat > costs\n",
    "\n",
    "gross_profit_per_unit = candidates_before_vat - costs\n",
    "gross_profit_total = gross_profit_per_unit * expected_units_daily\n",
    "\n",
    "# Penalties\n",
    "comp_gap_up = np.maximum(0, candidates - comp_prices)\n",
    "affordability_penalty = comp_gap_up * np.where(essential_factor == 1, 1.20, 0.60)\n",
    "instability_penalty = np.abs(candidates - last_prices) * 0.30\n",
    "stockout_penalty = np.where(df_opt['stockout_risk_14d'].values[:, np.newaxis] == 1,\n",
    "                            np.maximum(0, candidates - reg_prices) * 0.15,\n",
    "                            0)\n",
    "\n",
    "scores = np.where(\n",
    "    feasible_mask & valid_margin_mask,\n",
    "    gross_profit_total - affordability_penalty - instability_penalty - stockout_penalty,\n",
    "    -1e12\n",
    ")\n",
    "\n",
    "best_indices = np.argmax(scores, axis=1)\n",
    "df_opt['proposed_price'] = candidates[np.arange(len(df_opt)), best_indices]\n",
    "\n",
    "# Fallback if no feasible candidate\n",
    "no_feasible = ~np.any(feasible_mask & valid_margin_mask, axis=1)\n",
    "if np.any(no_feasible):\n",
    "    fallback_floor = np.maximum.reduce([\n",
    "        df_opt['regular_price'].values * 0.70,\n",
    "        df_opt['unit_cost'].values * (1 + df_opt['vat_rate'].values),\n",
    "        df_opt['last_price_paid'].values * (1 - MAX_DAILY_CHANGE_PCT),\n",
    "    ])\n",
    "    fallback_cap = np.minimum(\n",
    "        df_opt['last_price_paid'].values * (1 + MAX_DAILY_CHANGE_PCT),\n",
    "        np.where(df_opt['is_essential'].values == 1,\n",
    "                 df_opt['comp_price'].values * (1 + ESSENTIAL_MAX_GAP_PCT),\n",
    "                 np.inf)\n",
    "    )\n",
    "\n",
    "    fallback_candidates = np.column_stack([\n",
    "        np.floor(fallback_floor) + 0.0,\n",
    "        np.floor(fallback_floor) + 0.5,\n",
    "        np.floor(fallback_floor) + 0.9,\n",
    "        np.ceil(fallback_floor) + 0.0,\n",
    "        np.ceil(fallback_floor) + 0.5,\n",
    "        np.ceil(fallback_floor) + 0.9,\n",
    "    ])\n",
    "    fallback_ok = (fallback_candidates >= fallback_floor[:, None]) & (fallback_candidates <= fallback_cap[:, None])\n",
    "    fallback_candidates = np.where(fallback_ok, fallback_candidates, np.inf)\n",
    "    chosen_fallback = np.min(fallback_candidates, axis=1)\n",
    "\n",
    "    unresolved = np.isinf(chosen_fallback)\n",
    "    if np.any(unresolved):\n",
    "        fallback_unresolved = apply_rounding(fallback_floor[unresolved][:, np.newaxis]).ravel()\n",
    "        chosen_fallback[unresolved] = fallback_unresolved\n",
    "\n",
    "    df_opt.loc[no_feasible, 'proposed_price'] = chosen_fallback[no_feasible]\n",
    "\n",
    "# Final hard enforcement for requested rules\n",
    "proposed = df_opt['proposed_price'].values\n",
    "\n",
    "# Rule ending must be 0.00, 0.50, 0.90\n",
    "proposed = apply_rounding(proposed[:, np.newaxis]).ravel()\n",
    "\n",
    "# Essential fairness must hold (essential SKUs <= competitor * (1 + cap))\n",
    "essential_cap_series = np.where(\n",
    "    df_opt['is_essential'].values == 1,\n",
    "    df_opt['comp_price'].values * (1 + ESSENTIAL_MAX_GAP_PCT),\n",
    "    np.inf\n",
    ")\n",
    "violating_essential = (df_opt['is_essential'].values == 1) & (proposed > essential_cap_series)\n",
    "if np.any(violating_essential):\n",
    "    cap_vals = essential_cap_series[violating_essential]\n",
    "    cap_base = np.floor(cap_vals)\n",
    "    cap_90 = cap_base + 0.9\n",
    "    cap_50 = cap_base + 0.5\n",
    "    cap_00 = cap_base + 0.0\n",
    "    rounded_down = np.where(cap_90 <= cap_vals, cap_90, np.where(cap_50 <= cap_vals, cap_50, cap_00))\n",
    "    proposed[violating_essential] = rounded_down\n",
    "\n",
    "df_opt['proposed_price'] = proposed\n",
    "\n",
    "# Final metrics\n",
    "df_opt['proposed_price_before_vat'] = df_opt['proposed_price'] / (1 + df_opt['vat_rate'])\n",
    "df_opt['gross_profit_per_unit'] = df_opt['proposed_price_before_vat'] - df_opt['unit_cost']\n",
    "df_opt['expected_unit_sold'] = df_opt['avg_daily_demand_14d']\n",
    "df_opt['expected_gross_profit'] = df_opt['gross_profit_per_unit'] * df_opt['expected_unit_sold']\n",
    "df_opt['price_gap_vs_comp'] = df_opt['proposed_price'] - df_opt['comp_price']\n",
    "\n",
    "frac = df_opt['proposed_price'] - np.floor(df_opt['proposed_price'])\n",
    "df_opt['rule_allowed_ending'] = np.isclose(frac, 0.0, atol=1e-6) | np.isclose(frac, 0.5, atol=1e-6) | np.isclose(frac, 0.9, atol=1e-6)\n",
    "df_opt['rule_markdown_30pct'] = df_opt['proposed_price'] >= (df_opt['regular_price'] * 0.70)\n",
    "df_opt['rule_min_price_cost_vat'] = df_opt['proposed_price'] >= (df_opt['unit_cost'] * (1 + df_opt['vat_rate']))\n",
    "df_opt['rule_daily_fluctuation'] = (\n",
    "    (df_opt['proposed_price'] >= (df_opt['last_price_paid'] * (1 - MAX_DAILY_CHANGE_PCT))) &\n",
    "    (df_opt['proposed_price'] <= (df_opt['last_price_paid'] * (1 + MAX_DAILY_CHANGE_PCT)))\n",
    ")\n",
    "df_opt['rule_essential_fair'] = np.where(\n",
    "    df_opt['is_essential'] == 1,\n",
    "    df_opt['proposed_price'] <= df_opt['comp_price'] * (1 + ESSENTIAL_MAX_GAP_PCT),\n",
    "    True\n",
    ")\n",
    "\n",
    "print(f'   - Optimization completed in {time.time() - opt_start:.4f}s')\n",
    "\n",
    "# 7. Save Outputs\n",
    "print('[7/7] Saving results...')\n",
    "run_tag = int(time.time())\n",
    "output_path = os.path.join(OUTPUT_DIR, f'final_submission_social_balanced_v1_{run_tag}.csv')\n",
    "submission_df = df_opt[['ID', 'store_id', 'sku_id', 'date', 'proposed_price']].copy()\n",
    "submission_df['date'] = submission_df['date'].dt.strftime('%d/%m/%Y')\n",
    "submission_df.to_csv(output_path, index=False)\n",
    "\n",
    "analysis_output_path = os.path.join(OUTPUT_DIR, f'final_submission_analysis_social_balanced_v1_{run_tag}.csv')\n",
    "analysis_cols = [\n",
    "    'ID', 'store_id', 'sku_id', 'date', 'category', 'subcategory', 'is_essential',\n",
    "    'regular_price', 'comp_price', 'proposed_price', 'price_gap_vs_comp',\n",
    "    'proposed_price_before_vat', 'unit_cost', 'vat_rate', 'last_price_paid',\n",
    "    'expected_unit_sold', 'gross_profit_per_unit', 'expected_gross_profit',\n",
    "    'on_hand', 'expected_units_14d', 'stockout_risk_14d',\n",
    "    'rule_allowed_ending', 'rule_markdown_30pct', 'rule_min_price_cost_vat',\n",
    "    'rule_daily_fluctuation', 'rule_essential_fair'\n",
    "]\n",
    "df_opt[analysis_cols].to_csv(analysis_output_path, index=False)\n",
    "\n",
    "# Summary\n",
    "n_rows = len(df_opt)\n",
    "over_comp_count = int((df_opt['price_gap_vs_comp'] > 0).sum())\n",
    "essential_over_comp_count = int(((df_opt['is_essential'] == 1) & (df_opt['price_gap_vs_comp'] > 0)).sum())\n",
    "\n",
    "print('\\n' + '=' * 30)\n",
    "print(f'Done! Final submission saved to: {output_path}')\n",
    "print(f'Analysis file saved to: {analysis_output_path}')\n",
    "print(f'Expected total gross profit: {df_opt[\"expected_gross_profit\"].sum():,.2f}')\n",
    "print(f'Over competitor rows (all): {over_comp_count}/{n_rows}')\n",
    "print(f'Over competitor rows (essential): {essential_over_comp_count}/{int((df_opt[\"is_essential\"] == 1).sum())}')\n",
    "print(f'Rule pass - allowed ending: {int(df_opt[\"rule_allowed_ending\"].sum())}/{n_rows}')\n",
    "print(f'Rule pass - markdown <= 30%: {int(df_opt[\"rule_markdown_30pct\"].sum())}/{n_rows}')\n",
    "print(f'Rule pass - min price cost+VAT: {int(df_opt[\"rule_min_price_cost_vat\"].sum())}/{n_rows}')\n",
    "print(f'Rule pass - daily fluctuation: {int(df_opt[\"rule_daily_fluctuation\"].sum())}/{n_rows}')\n",
    "print(f'Rule pass - essential fairness: {int(df_opt[\"rule_essential_fair\"].sum())}/{n_rows}')\n",
    "print(f'Total Execution Time: {time.time() - global_start:.2f}s')\n",
    "print('=' * 30)\n",
    "print(submission_df.head(10).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
